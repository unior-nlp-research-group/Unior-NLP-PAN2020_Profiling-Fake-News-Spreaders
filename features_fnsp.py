import emoji
import string
import re
from emoji import UNICODE_EMOJI
from nltk.tokenize import TweetTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer


def count_emoji(text):
    return len([c for c in text if c in UNICODE_EMOJI])

def face_neutral_skeptical(text):
    return len([c for c in text if c in 'ğŸ¤ğŸ¤¨ğŸ˜ğŸ˜‘ğŸ˜¶ğŸ˜ğŸ˜’ğŸ™„ğŸ˜¬ğŸ¤¥'])


def face_concerned(text):
    return len([c for c in text if c in 'ğŸ˜•ğŸ˜ŸğŸ™â˜¹ğŸ˜®ğŸ˜¯ğŸ˜²ğŸ˜³ğŸ¥ºğŸ˜¦ğŸ˜§ğŸ˜¨ğŸ˜°ğŸ˜¥ğŸ˜¢ğŸ˜­ğŸ˜±ğŸ˜–ğŸ˜£ğŸ˜'])

def fire(text):
    return len([c for c in text if c in 'ğŸ¤‘ğŸ”¥ğŸ’¯ğŸ’¢ğŸ’¥ğŸ’£âœğŸ—£ğŸŒğŸŒğŸŒâ™¨ğŸš¨ğŸ¯ğŸ“£ğŸ“¢ğŸ“·ğŸ“¸ğŸ“°ğŸ—ğŸ’°ğŸ’¸ğŸ’³ğŸ’µğŸ’¶ğŸ’·'])


def face_smiling(text):
    return len([c for c in text if c in 'ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ¤£ğŸ˜‚ğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ŠğŸ˜‡'])


def face_affection(text):
    return len([c for c in text if c in 'ğŸ¥°ğŸ˜ğŸ¤©ğŸ˜˜ğŸ˜—â˜ºğŸ˜šğŸ˜™'])


def face_tongue(text):
    return len([c for c in text if c in 'ğŸ˜‹ğŸ˜›ğŸ˜œğŸ¤ªğŸ˜'])


def face_hand(text):
    return len([c for c in text if c in 'ğŸ¤—ğŸ¤­ğŸ¤«ğŸ¤”'])


def monkey_face(text):
    return len([c for c in text if c in 'ğŸ™ˆğŸ™‰ğŸ™Š'])


def emotions(text):
    return len([c for c in text if c in 'ğŸ’‹ğŸ’ŒğŸ’˜ğŸ’ğŸ’–ğŸ’—ğŸ’“ğŸ’ğŸ’•ğŸ’Ÿâ£ğŸ’”â¤ğŸ§¡ğŸ’›ğŸ’šğŸ’™ğŸ’œğŸ¤ğŸ–¤'])

def face(text):
    return len([c for c in text if c in 'ğŸ¤ğŸ¤¨ğŸ˜ğŸ˜‘ğŸ˜¶ğŸ˜ğŸ˜’ğŸ™„ğŸ˜¬ğŸ¤¥ğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜ğŸ˜†ğŸ˜…ğŸ¤£ğŸ˜‚ğŸ™‚ğŸ™ƒğŸ˜‰ğŸ˜ŠğŸ˜‡ğŸ˜‹ğŸ˜›ğŸ˜œğŸ¤ªğŸ˜ğŸ¤—ğŸ¤­ğŸ¤«ğŸ¤”ğŸ™ˆğŸ™‰ğŸ™Š' ])

def fl(text):
    return len([c for c in text if c in 'ğŸ‡¯ğŸ‡µ ğŸ‡°ğŸ‡· ğŸ‡©ğŸ‡ª ğŸ‡¨ğŸ‡³ ğŸ‡ºğŸ‡¸ ğŸ‡«ğŸ‡· ğŸ‡ªğŸ‡¸ ğŸ‡®ğŸ‡¹ ğŸ‡·ğŸ‡º ğŸ‡¬ğŸ‡§ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡­ğŸ‡µğŸ‡°'])

def preprocess(data):
    print('Preprocessing the Data')

    data['face_concerned'] = data['text'].apply(face_concerned)

    data['flags'] = data['text'].apply(fl)

    data['emotions'] = data['text'].apply(emotions, face_affection)

    data['face_neutral_skeptical'] = data['text'].apply(face_neutral_skeptical)

    data['fire'] = data['text'].apply(fire)

    data['emoji'] = data['text'].apply(count_emoji)

    data['url'] = data['text'].apply(lambda x: len(re.findall('http\S+', x)))

    data['space'] = data['text'].apply(lambda x: len(re.findall(' ', x)))

    data['words'] = data['text'].apply(lambda x: len(re.findall('[a-zA-Z]+', x)))

    data['CapitalLetter'] = data['text'].apply(lambda x: len(re.findall('[A-Z]', x)))

    data['Words_initial_capital'] = data['text'].apply(lambda x: len(re.findall(r"(?<!^)(?<!\. )[A-Z][a-z]+", x)))
    
    data['capital_WORD_count'] = data['text'].apply(lambda x: len(re.findall(r"\b[A-Z][A-Z\d]+\b", x)))

    data['digits'] = data['text'].apply(lambda x: len(re.findall('[0-9]+', x)))

    data['text_length'] = data['text'].apply(len)

    #data['curly_brackets'] = data['text'].apply(lambda x: len(re.findall('[\{\}]', x)))

    data['round_brackets'] = data['text'].apply(lambda x: len(re.findall('[\(\)]', x)))

    #data['quadre_brackets'] = data['text'].apply(lambda x: len(re.findall('\[\]', x)))

    #data['underscore'] = data['text'].apply(lambda x: len(re.findall('[_]', x)))

    data['question_mark'] = data['text'].apply(lambda x: len(re.findall('[?]', x)))

    data['exclamation_mark'] = data['text'].apply(lambda x: len(re.findall('[!]', x)))

    #data['dollar_mark'] = data['text'].apply(lambda x: len(re.findall('[$]', x)))

    #data['currency'] = data['text'].apply(lambda x: len(re.findall('(\d[0-9,.]+)', x)))

    #mon = ['$', 'USD', 'EUR', 'GBP', 'euro', 'euros', 'dollar', 'dollars', 'pound', 'nickel', 'dime', 'pounds', 'money', 'cash']

    #data['amount'] = data['text'].apply(lambda x: len(re.findall("(?=(\b" + '\\b|\\b'.join(mon) + r"\b))", x, flags=re.I)))

    data['ampersand_mark'] = data['text'].apply(lambda x: len(re.findall('&amp;', x))) #&amp;amp;

    data['retweet'] = data['text'].apply(lambda x: len(re.findall('RT', x)))

    data['hashtags'] = data['text'].apply(lambda x: len(re.findall('#HASHTAG#', x)))

    data['url'] = data['text'].apply(lambda x: len(re.findall('#URL#', x)))

    data['first_hash'] = data['text'].apply(lambda x: len(re.findall("^#HASHTAG#", x)))

    data['first_url'] = data['text'].apply(lambda x: len(re.findall("^#URL#", x)))

    data['mentions'] = data['text'].apply(lambda x: len(re.findall('[@]', x)))

    data['usr'] = data['text'].apply(lambda x: len(re.findall('#USER#', x)))

    data['slashes'] = data['text'].apply(lambda x: len(re.findall('[/,\\\\]', x)))

    data['citation'] = data['text'].apply(lambda x: len(re.findall('[\â€œ(.+?)\â€]', x))) 

    data['citation2'] = data['text'].apply(lambda x: len(re.findall('[\â€˜(.+?)\â€™]', x))) 

    data['ellipsis'] = data['text'].apply(lambda x: len(re.findall('â€¦', x)))

    data['operators'] = data['text'].apply(lambda x: len(re.findall('[+=\-*%<>^|â€“]', x)))

    data['punct'] = data['text'].apply(lambda x: len(re.findall('[\'\",.:;~`"â€¢]', x)))

    sens = ['watch', 'watch:', 'review', 'says', 'report','reports', 'latest','click', 'follow', 'listen', 'live', 'live now', 'video', 'videos', 'link', 'redirect', 'redirecta', 'direct', 'directa']

    data['adv'] = data['text'].apply(lambda x: len(re.findall("(?=(\b" + '\\b|\\b'.join(sens) + r"\b))", x, flags=re.I)))

    words = ["it", "its", "you", "your", "I", "u", "my", "she", "her", "he", "his", "they", "their", "mine", "myself", "ur", "yo", "tu", "tÃº", "mi", "toy", "Ã©l", "el", "tÃ¡s", "tas", "tÃ¡", "tamos", "tqm", "dtb", "xp", "besit2", "personalmente"]
        
    data['pron_pers'] = data['text'].apply(lambda x: len(re.findall("(?=(\b" + '\\b|\\b'.join(words) + r"\b))", x, flags=re.I)))

    return data


def wordvectorize(data):

    word_vectorizer = TfidfVectorizer(
    analyzer='word',
    token_pattern=r'\w{1,}',
    ngram_range=(1, 3)) #1,3 #2,3 #1,4
    word_vectorizer.fit_transform(data['text'])

"""
def charvectorize(data):

    char_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    #strip_accents='unicode',
    analyzer='char',
    #stop_words='english',
    ngram_range=(1, 3), #2,6 #1,3 #1,4 #1,5
    max_features=50000)
    char_vectorizer.fit_transform(data['text'])
"""